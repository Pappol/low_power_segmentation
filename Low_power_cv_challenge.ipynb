{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pappol/low_power_segmentation/blob/main/Low_power_cv_challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyTE4v_EmhC9"
      },
      "source": [
        "# Image segmentation with edge devices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYv5Swzhnotm"
      },
      "source": [
        "### Challenge\n",
        "The goal of this project is to develop a solution for image segmentation at the edge using the latest state of the art models on fÃ¬very low power computational units.\n",
        "\n",
        "This in our particular use case is based on UAV's images of natural disastes allowing for a very effective and low cost data collection in critical sitation as natural disasters such as earthquakes, volcanic eruptions and floods.\n",
        "\n",
        "The goal is to segment this type of images with the best accuracy and the lowest amount of inference time.\n",
        "\n",
        "### Hardware\n",
        "In this project we'll use Jetson Nano computing board 2GB. This is a small low power device developed by NVIDIA that is specifically created for AI.\n",
        "\n",
        "- Low cost of hardware: this allows to scale the system while drastically reducing the expenses\n",
        "-  Lower power consumption: power consumption\n",
        "- Reduced space: computational boards are very small in dimension and do not require active cooling, while GPUs can be very large and require to run other components, such as power supply which requires more space;\n",
        "- Durability: not requiring moving parts as fans, edge devices are more reliable especially in outdoor situations.\n",
        "\n",
        "### Difficulties\n",
        "\n",
        "This advantages comes at the cost of performance and constraints, the biggest one being the amount of memory available. This is a big limitation for the use of state of the art models that are very large and require a lot of memory to be loaded and run.\n",
        "A solution to this problem is to use models that are specifically designed to be run on edge devices, such as MobileNetV2, which is a very light model and quantization which allows to reduce the size of the model by reducing the precision of the weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiMdllKYml-G"
      },
      "source": [
        "## Dependecies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j3VV21YTnuA",
        "outputId": "2a15732c-6c13-463e-e728-fda49c160d2a"
      },
      "outputs": [],
      "source": [
        "#link drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSrZLBpYTpco"
      },
      "outputs": [],
      "source": [
        "#unpack dataset\n",
        "! unzip /content/drive/MyDrive/LPCVC_Train_Updated.zip\n",
        "! unzip /content/drive/MyDrive/LPCVC_Val.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRJLYwVWjLUM",
        "outputId": "db0a9ebd-6b13-425d-bd10-c785325c8cd5"
      },
      "outputs": [],
      "source": [
        "!pip install wandb -q\n",
        "! pip install -U accelerate -q\n",
        "! pip install -U transformers -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "Yzg-9PFbj211",
        "outputId": "0e2e0a5a-e193-4034-cb60-b23bdd2b4744"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "from transformers import TrainerCallback\n",
        "import numpy as np\n",
        "from transformers import AutoImageProcessor, MobileNetV2ForSemanticSegmentation, Trainer, TrainingArguments\n",
        "from PIL import Image\n",
        "import torch\n",
        "from matplotlib.colors import ListedColormap\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import random\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "from transformers import TrainerCallback\n",
        "\n",
        "\n",
        "import wandb\n",
        "wandb.init(project=\"cv-low_power_seg\")\n",
        "\n",
        "wandb.log({\"Test\": 1})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nR74vs3bmpEd"
      },
      "source": [
        "## Metrics for evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NCl2uF4SW5D"
      },
      "outputs": [],
      "source": [
        "class AccuracyTracker(object):\n",
        "    def __init__(self, n_classes):\n",
        "        self.n_classes = n_classes\n",
        "        self.confusion_matrix = np.zeros((n_classes, n_classes))\n",
        "\n",
        "    def reset(self):\n",
        "        self.confusion_matrix = np.zeros((self.n_classes, self.n_classes))\n",
        "\n",
        "    def _fast_hist(self, label_true, label_pred, n_class):\n",
        "        mask = (label_true >= 0) & (label_true < n_class)\n",
        "        hist = np.bincount(\n",
        "            n_class * label_true[mask].astype(int) + label_pred[mask],\n",
        "            minlength=n_class**2,\n",
        "        ).reshape(n_class, n_class)\n",
        "        return hist\n",
        "\n",
        "    def update(self, label_trues, label_preds):\n",
        "        for lt, lp in zip(label_trues, label_preds):\n",
        "            self.confusion_matrix += self._fast_hist(\n",
        "                lt.flatten(), lp.flatten(), self.n_classes\n",
        "            )\n",
        "\n",
        "    def get_scores(self):\n",
        "        \"\"\"Returns accuracy score evaluation result.\n",
        "        - overall accuracy\n",
        "        - mean accuracy\n",
        "        - mean IU\n",
        "        - fwavacc\n",
        "        \"\"\"\n",
        "        hist = self.confusion_matrix\n",
        "        self.acc = np.diag(hist).sum() / hist.sum()\n",
        "        acc_cls = np.diag(hist) / (hist.sum(axis=1) + 0.000000001)\n",
        "        self.acc_cls = np.nanmean(acc_cls)\n",
        "\n",
        "        with np.errstate(invalid='ignore'):\n",
        "            dice = 2*np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0))\n",
        "\n",
        "        self.mean_dice = np.nanmean(dice)\n",
        "        freq = hist.sum(axis=1) / hist.sum()\n",
        "        self.fwavacc = (freq[freq > 0] * dice[freq > 0]).sum()\n",
        "        self.cls_dice = dict(zip(range(self.n_classes), dice))\n",
        "\n",
        "        return {\n",
        "            \"Overall Acc: \\t\": self.acc,\n",
        "            \"Mean Acc : \\t\": self.acc_cls,\n",
        "            \"FreqW Acc : \\t\": self.fwavacc,\n",
        "            \"Mean Dice : \\t\": self.mean_dice,\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08O6U9iOm0Eo"
      },
      "source": [
        "## Dataset implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zt7caXd6m6MI"
      },
      "outputs": [],
      "source": [
        "class lpcv_dataset(Dataset):\n",
        "    def __init__(self, image_folder, label_folder, transform=None, augmentation=None):\n",
        "        self.image_folder = image_folder\n",
        "        self.label_folder = label_folder\n",
        "        self.image_filenames = sorted(os.listdir(image_folder))  # Sort filenames\n",
        "        self.transform = transform\n",
        "        self.augmentation = augmentation\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_filename = self.image_filenames[idx]\n",
        "        image_path = os.path.join(self.image_folder, image_filename)\n",
        "\n",
        "        # Generate corresponding label filename\n",
        "        label_path = os.path.join(self.label_folder, image_filename)\n",
        "\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        label = np.asarray(Image.open(label_path))[:,:,0]\n",
        "\n",
        "        if augmentation:\n",
        "            image, label = self.augmentation(image, label)\n",
        "\n",
        "        # Scale image pixel values to [0, 1] range\n",
        "        image = np.array(image) / 255.0\n",
        "\n",
        "        # Preprocess the image using the image_processor\n",
        "        inputs = image_processor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "        #remove the 3rd dimension from input\n",
        "        inputs[\"pixel_values\"] = inputs[\"pixel_values\"].squeeze(0)\n",
        "\n",
        "        return {\"pixel_values\": inputs[\"pixel_values\"], \"labels\": torch.tensor(label, dtype=torch.long)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBWlCfrXUD9_"
      },
      "outputs": [],
      "source": [
        "def augmentation(image, label, angle_range=15, target_size=(512, 512)):\n",
        "    #convert lable into PIL image\n",
        "    label = Image.fromarray(label)\n",
        "\n",
        "    #random horizontal flip\n",
        "    if random.random() > 0.8:\n",
        "        image = torchvision.transforms.functional.hflip(image)\n",
        "        #print lable type\n",
        "        label = torchvision.transforms.functional.hflip(label)\n",
        "\n",
        "    #random vertical flip\n",
        "    if random.random() > 0.5:\n",
        "        image = torchvision.transforms.functional.vflip(image)\n",
        "        label = torchvision.transforms.functional.vflip(label)\n",
        "\n",
        "    #convert label into numpy array\n",
        "    label = np.asarray(label)\n",
        "\n",
        "    return image, label\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyE0m_k0m_FC"
      },
      "source": [
        "## Model setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9OCV7ArnCF3"
      },
      "outputs": [],
      "source": [
        "class CustomTrainer(Trainer):\n",
        "    def __init__(self, *args, accuracy_tracker, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.accuracy_tracker = accuracy_tracker\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        loss = super().compute_loss(model, inputs, return_outputs=return_outputs)\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, model, inputs):\n",
        "        loss = super().training_step(model, inputs)\n",
        "        return loss\n",
        "\n",
        "    def evaluation_step(self, model, inputs):\n",
        "        loss, logits = super().evaluation_step(model, inputs)\n",
        "        self.accuracy_tracker.update(inputs[\"labels\"], logits.argmax(dim=1))\n",
        "        return loss, logits\n",
        "\n",
        "    def on_epoch_end(self, args, state, control, logs=None, **kwargs):\n",
        "      # Calculate and print accuracy metrics at the end of an epoch\n",
        "        accuracy_scores = self.accuracy_tracker.get_scores()\n",
        "        print(accuracy_scores)\n",
        "        super().on_epoch_end(args, state, control, logs=logs, **kwargs)\n",
        "\n",
        "class CustomCallback(TrainerCallback):\n",
        "    def __init__(self, trainer, accuracy_tracker, eval_dataset) -> None:\n",
        "        super().__init__()\n",
        "        self._trainer = trainer\n",
        "        self.accuracy_tracker = accuracy_tracker\n",
        "        self.eval_dataset = eval_dataset\n",
        "\n",
        "    def on_epoch_end(self, args, state, control, **kwargs):\n",
        "        if control.should_evaluate:\n",
        "            control_copy = deepcopy(control)\n",
        "\n",
        "            # Evaluate on the provided evaluation dataset\n",
        "            self._trainer.evaluate(eval_dataset=self.eval_dataset)\n",
        "\n",
        "            # Get label trues and label preds for accuracy tracking\n",
        "            label_trues = self._trainer.callback_metrics['label_ids']\n",
        "            label_preds = self._trainer.callback_metrics['predictions']\n",
        "            self.accuracy_tracker.update(label_trues, label_preds)\n",
        "\n",
        "            # Get accuracy scores\n",
        "            accuracy_scores = self.accuracy_tracker.get_scores()\n",
        "\n",
        "            # Log metrics using WandB\n",
        "            wandb.log(accuracy_scores, step=self._trainer.state.epoch)\n",
        "\n",
        "            return control_copy\n",
        "\n",
        "\n",
        "def test_model(img_path, save_path, model, preprocess):\n",
        "\n",
        "    image = Image.open(img_path)\n",
        "    inputs = image_processor(images=image, return_tensors=\"pt\").to('cuda')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "\n",
        "    # Post-process logits into segmentation mask\n",
        "    segmentation_mask = torch.argmax(logits, dim=1)\n",
        "\n",
        "    # Convert segmentation mask to colored image (assuming 14 color channels)\n",
        "    colored_image = ListedColormap(colors)(segmentation_mask[0].cpu().numpy())\n",
        "\n",
        "    #save the image\n",
        "    Image.fromarray((colored_image * 255).astype(np.uint8)).save(f\"segmented_image_.png\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzJpWXJanEvR"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-PTf8qolnNR",
        "outputId": "fbe6598e-0af8-4661-ba3b-eb00a8521f02"
      },
      "outputs": [],
      "source": [
        "categories = [\"background\", \"avalanche\",\n",
        "              \"building_undamaged\", \"building_damaged\",\n",
        "              \"cracks/fissure/subsidence\", \"debris/mud/rock flow\",\n",
        "              \"fire/flare\", \"flood/water/river/sea\",\n",
        "              \"ice_jam_flow\", \"lava_flow\",\n",
        "              \"person\", \"pyroclastic_flow\",\n",
        "              \"road/railway/bridge\", \"vehicle\"]\n",
        "\n",
        "colors = ['black', 'white', 'pink', 'yellow', 'orange', 'brown',\n",
        "          'red', 'blue', 'navy', 'orange', 'cyan', 'gray',\n",
        "          'magenta']\n",
        "\n",
        "#import changing initial resulution and number of classes\n",
        "image_processor = AutoImageProcessor.from_pretrained(\"google/deeplabv3_mobilenet_v2_1.0_513\",\n",
        "                                                     num_labels=len(categories),\n",
        "                                                     ignore_mismatched_sizes=True,\n",
        "                                                     crop_size=(512, 512))\n",
        "\n",
        "model = MobileNetV2ForSemanticSegmentation.from_pretrained(\"google/deeplabv3_mobilenet_v2_1.0_513\",\n",
        "                                                           num_labels=len(categories),\n",
        "                                                           ignore_mismatched_sizes=True,\n",
        "                                                           image_size=(512, 512))\n",
        "\n",
        "\n",
        "\n",
        "image_folder=\"LPCVC_Train_Updated/LPCVC_Train_Updated/LPCVC_Train_Updated/IMG/train\"\n",
        "label_folder=\"LPCVC_Train_Updated/LPCVC_Train_Updated/LPCVC_Train_Updated/GT_Updated/train\"\n",
        "\n",
        "val_folder=\"LPCVC_Val/LPCVC_Val/IMG/val\"\n",
        "val_label_folder=\"LPCVC_Val/LPCVC_Val/GT/val\"\n",
        "\n",
        "train_dataset = lpcv_dataset(image_folder, label_folder, augmentation=augmentation)\n",
        "\n",
        "accuracy_tracker = AccuracyTracker(len(categories))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "vlVfCDMLXPcs",
        "outputId": "532cf93a-2cea-46c8-f105-84455991b264"
      },
      "outputs": [],
      "source": [
        "custom_trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args= TrainingArguments(output_dir=\"test_trainer\", num_train_epochs=10, per_device_train_batch_size=8),\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset= lpcv_dataset(val_folder, val_label_folder),\n",
        "    accuracy_tracker= accuracy_tracker\n",
        ")\n",
        "\n",
        "eval_callback = CustomCallback(trainer=custom_trainer, accuracy_tracker=accuracy_tracker, eval_dataset=lpcv_dataset(val_folder, val_label_folder))\n",
        "\n",
        "custom_trainer.add_callback(eval_callback)\n",
        "\n",
        "custom_trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMCSklaznJeQ"
      },
      "source": [
        "## Model quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Usually, on edge devices, operations with floating points are very computationally demanding. Quantization is one of the most effective Tiny machine learning techniques for reducing computing time and energy needed by neural networks. The weights and activation tensors in neural network quantization are stored in lower bit precision than the 16 or 32 bits they are typically trained in. While the computational cost for matrix multiplication lowers by a factor of 16 when switching from 32 to 8 bits, the memory overhead for storing tensors decreases by a factor of 4. The main operations in neural networks are matrix multiplications, in \\cref{fig:calc} is depicted how  $y = Wx + b$ is calculated.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1V0sQ3RpodWxFO0_Tan8rYt-9VeRb9CLm' style=\"width: 70%;\">\n",
        "\n",
        "The operation is composed of: *processing elements* $C_{n,m}$, *accumulators* ($A_n$), *weights* $W_{n,m}$, and *bias* $b_n$.\n",
        "\n",
        "When $C_{n,m}=W_{n,m}*x_m$, the result of the multiplication is represented as:\n",
        "\n",
        "\\begin{equation}\n",
        "    A_n=b_n+\\sum_{m}C_{n,m}\n",
        "\\end{equation}\n",
        "\n",
        "A floating point can be easily converted into a fixed point with simple multiplication. $\\hat{x}$ can be approximated as a scalar multiplied by a vector of integer values: $\\hat{X} = S_x\\cdot X_{int} \\approx\tX$. By quantifying the weights and activation, we can write the quantified version of the accumulation equation as:\n",
        "\n",
        "\\begin{equation}\n",
        "    A_n=b_n+s_w s_x\\sum_{m}W^{int}_{n,m} x^{int}_{n,m}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=10DHXc9PvojhMcwNTjSS0LsIrL8Z37dWs' style=\"width: 70%;\">\n",
        "\n",
        "The process of quantization should not only simplify the operations but also reduces the size of the model too. In this case, the net is trained normally and then is applied quantization, this process is called knowledge distillation. It allows for a lower impact on performances since the reduction is applied only after training.  Knowledge distillation is a very used process in TinyML because it is used to transfer the training process to a smaller model reducing the degradation caused by the reduction. Quantization is a well-known strategy and it is used in different fields from signal processing to compression and its results are valid.\n",
        "\n",
        "https://arxiv.org/pdf/2106.08295.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VYTENIEZMHc"
      },
      "outputs": [],
      "source": [
        "#QUANTIZE THE MODEL\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Convert the model to quantized version\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "\n",
        "# Save the model\n",
        "torch.save(quantized_model, 'model_quantized.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6INl9leBhEQ8"
      },
      "source": [
        "##TEST WITH ONE IMAGE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLl1GReBeVOa"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "folderPath = '/content/LPCVC_Train_Updated/LPCVC_Train_Updated/LPCVC_Train_Updated'\n",
        "seed= random.randint(0, 999)\n",
        "print(f'{seed:03d}')\n",
        "imagePath = f'{folderPath}/IMG/train/train_0{seed:03d}.png'\n",
        "\n",
        "#Visualize image\n",
        "img = Image.open(imagePath)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NHtADtJe0Du"
      },
      "outputs": [],
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "def printSEGMENTATION(filepath):\n",
        "    # Define the colors for each value in the matrix\n",
        "    cmap = ListedColormap(colors)\n",
        "\n",
        "    mask = np.asarray(Image.open(filepath))[:,:,0]\n",
        "    print(mask.shape)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(mask, vmin=0, vmax=13)\n",
        "    cbar = plt.colorbar(ticks=np.arange(14), label='Categories')\n",
        "    # cbar = plt.colorbar(ticks=np.arange(14), cmap = cmap, label='Categories')\n",
        "    cbar.ax.set_yticklabels(categories)\n",
        "    plt.show()\n",
        "\n",
        "printSEGMENTATION(f\"{folderPath}/GT_Updated/train/train_0{seed:03d}.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwdGTUQXfRKT"
      },
      "outputs": [],
      "source": [
        "test_model(imagePath, \"test.png\", model, image_processor )\n",
        "printSEGMENTATION(\"/content/segmented_image_.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbaRZhbOqA3N"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "nR74vs3bmpEd"
      ],
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
