{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "nR74vs3bmpEd"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pappol/low_power_segmentation/blob/main/Low_power_cv_challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image segmentation with edge devices"
      ],
      "metadata": {
        "id": "yyTE4v_EmhC9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Challenge\n",
        "The goal of this project is to develop a solution for image segmentation at the edge using the latest state of the art models on fìvery low power computational units.\n",
        "\n",
        "This in our particular use case is based on UAV's images of natural disastes allowing for a very effective and low cost data collection in critical sitation as natural disasters such as earthquakes, volcanic eruptions and floods.\n",
        "\n",
        "The goal is to segment this type of images with the best accuracy and the lowest amount of inference time.\n",
        "\n",
        "### Hardware\n",
        "In this project we'll use Jetson Nano computing board 2GB. This is a small low power device developed by NVIDIA that is specifically created for AI.\n",
        "\n",
        "- Low cost of hardware: this allows to scale the system while drastically reducing the expenses\n",
        "-  Lower power consumption: power consumption\n",
        "- Reduced space: computational boards are very small in dimension and do not require active cooling, while GPUs can be very large and require to run other components, such as power supply which requires more space;\n",
        "- Durability: not requiring moving parts as fans, edge devices are more reliable especially in outdoor situations.\n",
        "\n",
        "### Difficulties\n",
        "\n",
        "This advantages comes at the cost of performance and constraints"
      ],
      "metadata": {
        "id": "AYv5Swzhnotm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependecies"
      ],
      "metadata": {
        "id": "PiMdllKYml-G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j3VV21YTnuA",
        "outputId": "2a15732c-6c13-463e-e728-fda49c160d2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#link drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#unpack dataset\n",
        "! unzip /content/drive/MyDrive/LPCVC_Train_Updated.zip\n",
        "! unzip /content/drive/MyDrive/LPCVC_Val.zip\n"
      ],
      "metadata": {
        "id": "JSrZLBpYTpco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -q\n",
        "! pip install -U accelerate -q\n",
        "! pip install -U transformers -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRJLYwVWjLUM",
        "outputId": "db0a9ebd-6b13-425d-bd10-c785325c8cd5"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.8/218.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.22.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.32.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "from transformers import TrainerCallback\n",
        "import numpy as np\n",
        "from transformers import AutoImageProcessor, MobileNetV2ForSemanticSegmentation, Trainer, TrainingArguments\n",
        "from PIL import Image\n",
        "import torch\n",
        "from matplotlib.colors import ListedColormap\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import random\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "from transformers import TrainerCallback\n",
        "\n",
        "\n",
        "import wandb\n",
        "wandb.init(project=\"cv-low_power_seg\")\n",
        "\n",
        "wandb.log({\"Test\": 1})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "Yzg-9PFbj211",
        "outputId": "0e2e0a5a-e193-4034-cb60-b23bdd2b4744"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230830_134048-ezkskhrh</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/pappol/cv-low_power_seg/runs/ezkskhrh' target=\"_blank\">honest-wood-5</a></strong> to <a href='https://wandb.ai/pappol/cv-low_power_seg' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/pappol/cv-low_power_seg' target=\"_blank\">https://wandb.ai/pappol/cv-low_power_seg</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/pappol/cv-low_power_seg/runs/ezkskhrh' target=\"_blank\">https://wandb.ai/pappol/cv-low_power_seg/runs/ezkskhrh</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics for evaluation\n"
      ],
      "metadata": {
        "id": "nR74vs3bmpEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AccuracyTracker(object):\n",
        "    def __init__(self, n_classes):\n",
        "        self.n_classes = n_classes\n",
        "        self.confusion_matrix = np.zeros((n_classes, n_classes))\n",
        "\n",
        "    def reset(self):\n",
        "        self.confusion_matrix = np.zeros((self.n_classes, self.n_classes))\n",
        "\n",
        "    def _fast_hist(self, label_true, label_pred, n_class):\n",
        "        mask = (label_true >= 0) & (label_true < n_class)\n",
        "        hist = np.bincount(\n",
        "            n_class * label_true[mask].astype(int) + label_pred[mask],\n",
        "            minlength=n_class**2,\n",
        "        ).reshape(n_class, n_class)\n",
        "        return hist\n",
        "\n",
        "    def update(self, label_trues, label_preds):\n",
        "        for lt, lp in zip(label_trues, label_preds):\n",
        "            self.confusion_matrix += self._fast_hist(\n",
        "                lt.flatten(), lp.flatten(), self.n_classes\n",
        "            )\n",
        "\n",
        "    def get_scores(self):\n",
        "        \"\"\"Returns accuracy score evaluation result.\n",
        "        - overall accuracy\n",
        "        - mean accuracy\n",
        "        - mean IU\n",
        "        - fwavacc\n",
        "        \"\"\"\n",
        "        hist = self.confusion_matrix\n",
        "        self.acc = np.diag(hist).sum() / hist.sum()\n",
        "        acc_cls = np.diag(hist) / (hist.sum(axis=1) + 0.000000001)\n",
        "        self.acc_cls = np.nanmean(acc_cls)\n",
        "\n",
        "        with np.errstate(invalid='ignore'):\n",
        "            dice = 2*np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0))\n",
        "\n",
        "        self.mean_dice = np.nanmean(dice)\n",
        "        freq = hist.sum(axis=1) / hist.sum()\n",
        "        self.fwavacc = (freq[freq > 0] * dice[freq > 0]).sum()\n",
        "        self.cls_dice = dict(zip(range(self.n_classes), dice))\n",
        "\n",
        "        return {\n",
        "            \"Overall Acc: \\t\": self.acc,\n",
        "            \"Mean Acc : \\t\": self.acc_cls,\n",
        "            \"FreqW Acc : \\t\": self.fwavacc,\n",
        "            \"Mean Dice : \\t\": self.mean_dice,\n",
        "        }\n"
      ],
      "metadata": {
        "id": "1NCl2uF4SW5D"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset implementation"
      ],
      "metadata": {
        "id": "08O6U9iOm0Eo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class lpcv_dataset(Dataset):\n",
        "    def __init__(self, image_folder, label_folder, transform=None, augmentation=None):\n",
        "        self.image_folder = image_folder\n",
        "        self.label_folder = label_folder\n",
        "        self.image_filenames = sorted(os.listdir(image_folder))  # Sort filenames\n",
        "        self.transform = transform\n",
        "        self.augmentation = augmentation\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_filename = self.image_filenames[idx]\n",
        "        image_path = os.path.join(self.image_folder, image_filename)\n",
        "\n",
        "        # Generate corresponding label filename\n",
        "        label_path = os.path.join(self.label_folder, image_filename)\n",
        "\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        label = np.asarray(Image.open(label_path))[:,:,0]\n",
        "\n",
        "        if augmentation:\n",
        "            image, label = self.augmentation(image, label)\n",
        "\n",
        "        # Scale image pixel values to [0, 1] range\n",
        "        image = np.array(image) / 255.0\n",
        "\n",
        "        # Preprocess the image using the image_processor\n",
        "        inputs = image_processor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "        #remove the 3rd dimension from input\n",
        "        inputs[\"pixel_values\"] = inputs[\"pixel_values\"].squeeze(0)\n",
        "\n",
        "        return {\"pixel_values\": inputs[\"pixel_values\"], \"labels\": torch.tensor(label, dtype=torch.long)}"
      ],
      "metadata": {
        "id": "Zt7caXd6m6MI"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augmentation(image, label, angle_range=15, target_size=(512, 512)):\n",
        "    #convert lable into PIL image\n",
        "    label = Image.fromarray(label)\n",
        "\n",
        "    #random horizontal flip\n",
        "    if random.random() > 0.8:\n",
        "        image = torchvision.transforms.functional.hflip(image)\n",
        "        #print lable type\n",
        "        label = torchvision.transforms.functional.hflip(label)\n",
        "\n",
        "    #random vertical flip\n",
        "    if random.random() > 0.5:\n",
        "        image = torchvision.transforms.functional.vflip(image)\n",
        "        label = torchvision.transforms.functional.vflip(label)\n",
        "\n",
        "    #convert label into numpy array\n",
        "    label = np.asarray(label)\n",
        "\n",
        "    return image, label\n",
        "\n"
      ],
      "metadata": {
        "id": "bBWlCfrXUD9_"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model setup"
      ],
      "metadata": {
        "id": "hyE0m_k0m_FC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTrainer(Trainer):\n",
        "    def __init__(self, *args, accuracy_tracker, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.accuracy_tracker = accuracy_tracker\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        loss = super().compute_loss(model, inputs, return_outputs=return_outputs)\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, model, inputs):\n",
        "        loss = super().training_step(model, inputs)\n",
        "        return loss\n",
        "\n",
        "    def evaluation_step(self, model, inputs):\n",
        "        loss, logits = super().evaluation_step(model, inputs)\n",
        "        self.accuracy_tracker.update(inputs[\"labels\"], logits.argmax(dim=1))\n",
        "        return loss, logits\n",
        "\n",
        "    def on_epoch_end(self, args, state, control, logs=None, **kwargs):\n",
        "      # Calculate and print accuracy metrics at the end of an epoch\n",
        "        accuracy_scores = self.accuracy_tracker.get_scores()\n",
        "        print(accuracy_scores)\n",
        "        super().on_epoch_end(args, state, control, logs=logs, **kwargs)\n",
        "\n",
        "class CustomCallback(TrainerCallback):\n",
        "    def __init__(self, trainer, accuracy_tracker, eval_dataset) -> None:\n",
        "        super().__init__()\n",
        "        self._trainer = trainer\n",
        "        self.accuracy_tracker = accuracy_tracker\n",
        "        self.eval_dataset = eval_dataset\n",
        "\n",
        "    def on_epoch_end(self, args, state, control, **kwargs):\n",
        "        if control.should_evaluate:\n",
        "            control_copy = deepcopy(control)\n",
        "\n",
        "            # Evaluate on the provided evaluation dataset\n",
        "            self._trainer.evaluate(eval_dataset=self.eval_dataset)\n",
        "\n",
        "            # Get label trues and label preds for accuracy tracking\n",
        "            label_trues = self._trainer.callback_metrics['label_ids']\n",
        "            label_preds = self._trainer.callback_metrics['predictions']\n",
        "            self.accuracy_tracker.update(label_trues, label_preds)\n",
        "\n",
        "            # Get accuracy scores\n",
        "            accuracy_scores = self.accuracy_tracker.get_scores()\n",
        "\n",
        "            # Log metrics using WandB\n",
        "            wandb.log(accuracy_scores, step=self._trainer.state.epoch)\n",
        "\n",
        "            return control_copy\n",
        "\n",
        "\n",
        "def test_model(img_path, save_path, model, preprocess):\n",
        "\n",
        "    image = Image.open(img_path)\n",
        "    inputs = image_processor(images=image, return_tensors=\"pt\").to('cuda')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "\n",
        "    # Post-process logits into segmentation mask\n",
        "    segmentation_mask = torch.argmax(logits, dim=1)\n",
        "\n",
        "    # Convert segmentation mask to colored image (assuming 14 color channels)\n",
        "    colored_image = ListedColormap(colors)(segmentation_mask[0].cpu().numpy())\n",
        "\n",
        "    #save the image\n",
        "    Image.fromarray((colored_image * 255).astype(np.uint8)).save(f\"segmented_image_.png\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n9OCV7ArnCF3"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "SzJpWXJanEvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categories = [\"background\", \"avalanche\",\n",
        "              \"building_undamaged\", \"building_damaged\",\n",
        "              \"cracks/fissure/subsidence\", \"debris/mud/rock flow\",\n",
        "              \"fire/flare\", \"flood/water/river/sea\",\n",
        "              \"ice_jam_flow\", \"lava_flow\",\n",
        "              \"person\", \"pyroclastic_flow\",\n",
        "              \"road/railway/bridge\", \"vehicle\"]\n",
        "\n",
        "colors = ['black', 'white', 'pink', 'yellow', 'orange', 'brown',\n",
        "          'red', 'blue', 'navy', 'orange', 'cyan', 'gray',\n",
        "          'magenta']\n",
        "\n",
        "#import changing initial resulution and number of classes\n",
        "image_processor = AutoImageProcessor.from_pretrained(\"google/deeplabv3_mobilenet_v2_1.0_513\",\n",
        "                                                     num_labels=len(categories),\n",
        "                                                     ignore_mismatched_sizes=True,\n",
        "                                                     crop_size=(512, 512))\n",
        "\n",
        "model = MobileNetV2ForSemanticSegmentation.from_pretrained(\"google/deeplabv3_mobilenet_v2_1.0_513\",\n",
        "                                                           num_labels=len(categories),\n",
        "                                                           ignore_mismatched_sizes=True,\n",
        "                                                           image_size=(512, 512))\n",
        "\n",
        "\n",
        "\n",
        "image_folder=\"LPCVC_Train_Updated/LPCVC_Train_Updated/LPCVC_Train_Updated/IMG/train\"\n",
        "label_folder=\"LPCVC_Train_Updated/LPCVC_Train_Updated/LPCVC_Train_Updated/GT_Updated/train\"\n",
        "\n",
        "val_folder=\"LPCVC_Val/LPCVC_Val/IMG/val\"\n",
        "val_label_folder=\"LPCVC_Val/LPCVC_Val/GT/val\"\n",
        "\n",
        "train_dataset = lpcv_dataset(image_folder, label_folder, augmentation=augmentation)\n",
        "\n",
        "accuracy_tracker = AccuracyTracker(len(categories))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-PTf8qolnNR",
        "outputId": "fbe6598e-0af8-4661-ba3b-eb00a8521f02"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of MobileNetV2ForSemanticSegmentation were not initialized from the model checkpoint at google/deeplabv3_mobilenet_v2_1.0_513 and are newly initialized because the shapes did not match:\n",
            "- segmentation_head.classifier.convolution.weight: found shape torch.Size([21, 256, 1, 1]) in the checkpoint and torch.Size([14, 256, 1, 1]) in the model instantiated\n",
            "- segmentation_head.classifier.convolution.bias: found shape torch.Size([21]) in the checkpoint and torch.Size([14]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "custom_trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args= TrainingArguments(output_dir=\"test_trainer\", num_train_epochs=10, per_device_train_batch_size=8),\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset= lpcv_dataset(val_folder, val_label_folder),\n",
        "    accuracy_tracker= accuracy_tracker\n",
        ")\n",
        "\n",
        "eval_callback = CustomCallback(trainer=custom_trainer, accuracy_tracker=accuracy_tracker, eval_dataset=lpcv_dataset(val_folder, val_label_folder))\n",
        "\n",
        "custom_trainer.add_callback(eval_callback)\n",
        "\n",
        "custom_trainer.train()"
      ],
      "metadata": {
        "id": "vlVfCDMLXPcs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "532cf93a-2cea-46c8-f105-84455991b264"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='317' max='1280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 317/1280 06:04 < 18:33, 0.86 it/s, Epoch 2.47/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model quantization"
      ],
      "metadata": {
        "id": "OMCSklaznJeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#QUANTIZE THE MODEL\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Convert the model to quantized version\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "\n",
        "# Save the model\n",
        "torch.save(quantized_model, 'model_quantized.pt')"
      ],
      "metadata": {
        "id": "4VYTENIEZMHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TEST WITH ONE IMAGE"
      ],
      "metadata": {
        "id": "6INl9leBhEQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "folderPath = '/content/LPCVC_Train_Updated/LPCVC_Train_Updated/LPCVC_Train_Updated'\n",
        "seed= random.randint(0, 999)\n",
        "print(f'{seed:03d}')\n",
        "imagePath = f'{folderPath}/IMG/train/train_0{seed:03d}.png'\n",
        "\n",
        "#Visualize image\n",
        "img = Image.open(imagePath)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pLl1GReBeVOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "def printSEGMENTATION(filepath):\n",
        "    # Define the colors for each value in the matrix\n",
        "    cmap = ListedColormap(colors)\n",
        "\n",
        "    mask = np.asarray(Image.open(filepath))[:,:,0]\n",
        "    print(mask.shape)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(mask, vmin=0, vmax=13)\n",
        "    cbar = plt.colorbar(ticks=np.arange(14), label='Categories')\n",
        "    # cbar = plt.colorbar(ticks=np.arange(14), cmap = cmap, label='Categories')\n",
        "    cbar.ax.set_yticklabels(categories)\n",
        "    plt.show()\n",
        "\n",
        "printSEGMENTATION(f\"{folderPath}/GT_Updated/train/train_0{seed:03d}.png\")"
      ],
      "metadata": {
        "id": "8NHtADtJe0Du"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model(imagePath, \"test.png\", model, image_processor )\n",
        "printSEGMENTATION(\"/content/segmented_image_.png\")"
      ],
      "metadata": {
        "id": "bwdGTUQXfRKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IbaRZhbOqA3N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}